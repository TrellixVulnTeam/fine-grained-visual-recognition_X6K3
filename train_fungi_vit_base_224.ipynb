{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-amount",
   "metadata": {
    "id": "prerequisite-removal"
   },
   "source": [
    "# Fine-tune Network on Danish Fungi 2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-liechtenstein",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "olympic-table",
    "outputId": "517462e8-03e7-4bc8-df39-76bc7ef6d105"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.core import models, metrics, training, data, loss_functions\n",
    "from src.utils import nb_setup\n",
    "from src.dev import experiments as exp\n",
    "\n",
    "DATA_DIR = 'data/danish_fungi_dataset/'\n",
    "TRAIN_SET_DIR = 'train_resized'\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "nb_setup.init()\n",
    "nb_setup.set_random_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b38efb-e5e5-4cdf-91e6-2f3630b35769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training \n",
    "config = exp.create_config(\n",
    "    data='df2020',\n",
    "    model='vit_base_224',\n",
    "    loss='ce',\n",
    "    opt='sgd',\n",
    "    no_epochs=30,\n",
    "    batch_size=64,\n",
    "    total_batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    weight='none',\n",
    "    dataset='mini',\n",
    "    scheduler='reduce_lr_on_plateau',\n",
    "    # note=''\n",
    ")\n",
    "\n",
    "# include configuration from model\n",
    "_model_config = models.get_model(config.model, pretrained=False).pretrained_config\n",
    "config.update(_model_config)\n",
    "\n",
    "# save config file\n",
    "config.save(DATA_DIR + config.specs_name)\n",
    "\n",
    "# create loss, optimizer and scheduler functions\n",
    "loss_fn = loss_functions.LOSSES[config.loss]\n",
    "weight_fn = loss_functions.WEIGHTING[config.weight]\n",
    "opt_fn = training.OPTIMIZERS[config.opt]\n",
    "sched_fn = training.SCHEDULERS[config.scheduler]\n",
    "\n",
    "DATASETS = {\n",
    "    'full': ('DF20-train_metadata_PROD.csv', 'DF20-public_test_metadata_PROD.csv'),\n",
    "    'mini': ('DF20M-train_metadata_PROD.csv', 'DF20M-public_test_metadata_PROD.csv')\n",
    "}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-neighbor",
   "metadata": {
    "id": "express-soldier"
   },
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-enforcement",
   "metadata": {
    "id": "dangerous-marijuana"
   },
   "outputs": [],
   "source": [
    "# load metadata\n",
    "train_df = pd.read_csv(DATA_DIR + DATASETS[config.dataset][0])\n",
    "valid_df = pd.read_csv(DATA_DIR + DATASETS[config.dataset][1])\n",
    "\n",
    "classes = np.unique(train_df['scientificName'])\n",
    "no_classes = len(classes)\n",
    "assert no_classes == len(np.unique(valid_df['scientificName']))\n",
    "print(f'No classes: {no_classes}')\n",
    "print(f'Train set length: {len(train_df):,d}')\n",
    "print(f'Validation set length: {len(valid_df):,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864c1d8-ac04-4d9b-8d26-dc89ce5d1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transforms\n",
    "train_tfms, valid_tfms = data.get_transforms(\n",
    "    size=config.input_size, mean=config.image_mean,\n",
    "    std=config.image_std)\n",
    "\n",
    "# create data loaders\n",
    "trainloader = data.get_dataloader(\n",
    "    train_df, img_path_col='image_path', label_col='scientificName',\n",
    "    path=DATA_DIR + TRAIN_SET_DIR, transforms=train_tfms, labels=classes,\n",
    "    batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "validloader = data.get_dataloader(\n",
    "    valid_df, img_path_col='image_path', label_col='scientificName',\n",
    "    path=DATA_DIR + TRAIN_SET_DIR, transforms=valid_tfms, labels=classes,\n",
    "    batch_size=config.batch_size, shuffle=False, num_workers=4)\n",
    "assert trainloader.dataset._label2id == validloader.dataset._label2id\n",
    "\n",
    "trainloader.dataset.show_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58a5c7-c80f-4f10-9c85-70ba3f443572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f5dd40c-1ec5-4455-ad59-a58819ddf0fd",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4029fb-e28d-4aa2-9947-d5be6267770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = models.get_model(config.model, no_classes, pretrained=True)\n",
    "assert np.all([param.requires_grad for param in model.parameters()])\n",
    "\n",
    "# create loss\n",
    "freq = train_df['scientificName'].value_counts()[trainloader.dataset.labels].values\n",
    "weights = weight_fn(freq)\n",
    "criterion = loss_fn(weight=torch.Tensor(weights).to(device) if weights is not None else None)\n",
    "\n",
    "# create trainer\n",
    "trainer = training.Trainer(\n",
    "    model,\n",
    "    trainloader,\n",
    "    criterion,\n",
    "    opt_fn,\n",
    "    sched_fn,\n",
    "    validloader=validloader,\n",
    "    accumulation_steps=config.total_batch_size // config.batch_size,\n",
    "    path=DATA_DIR,\n",
    "    model_filename=config.model_name,\n",
    "    history_filename=config.history_file,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd4e2f-4222-4303-bc55-2bd597aabad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train(no_epochs=config.no_epochs, lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c656f-e25f-4c98-afdd-9c291daa8cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find learning rate\n",
    "# lr_finder = trainer.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc136e2f-4584-44e7-b72b-9e3eca32db61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "05b_country_f1_score_on_CLEF2021.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
