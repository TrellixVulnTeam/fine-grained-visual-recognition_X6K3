{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d07377b-2922-46e6-b1de-e4b4c5ef3749",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 03 - Danish Fungi - Train Classifier on Cropped Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad7e78f-46b8-423a-8d86-6c3b428e653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b40231-2513-4e7d-b5eb-4c3f10b323ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.core import models, training, data, metrics, loss_functions\n",
    "from src.utils import nb_setup, visualization as viz\n",
    "from src.dev import experiments as exp\n",
    "\n",
    "BBOX_DATA_DIR = '03_informed_augmentation/data_fungi/'\n",
    "DATA_DIR = 'data/danish_fungi_dataset/'\n",
    "TRAIN_SET_DIR = 'train_resized'\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "nb_setup.init()\n",
    "nb_setup.set_random_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ffdefa-fd30-4fb1-bc74-f11e69d22a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIONS = ['crop', 'full and crop', 'full and crop (random)',\n",
    "           'full and crop (6 channels)', 'full and crop (mixup)']\n",
    "OPTION = 'full and crop'\n",
    "assert OPTION in OPTIONS\n",
    "\n",
    "# loss arguments\n",
    "GAMMA = None\n",
    "BETA = 0.999\n",
    "loss_kwg = {}\n",
    "if GAMMA is not None:\n",
    "    loss_kwg['gamma'] = float(GAMMA)\n",
    "weight_kwg = {}\n",
    "if BETA is not None:\n",
    "    weight_kwg['beta'] = float(BETA)\n",
    "\n",
    "# mixup arguments\n",
    "ALPHA = 0.5\n",
    "MIXUP_P = 0.25\n",
    "mixup_kwg = {}\n",
    "if OPTION == 'full and crop (mixup)':\n",
    "    mixup_kwg['alpha'] = float(ALPHA)\n",
    "    mixup_kwg['mixup_p'] = float(MIXUP_P)\n",
    "\n",
    "no_epochs = 30\n",
    "if OPTION == 'full and crop':\n",
    "    no_epochs = no_epochs // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1b81d0-0f19-4f5a-a9ce-7a54a2d822de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training \n",
    "config = exp.create_config(\n",
    "    data='df2020',\n",
    "    model='efficientnet_b0',\n",
    "    loss='ce',\n",
    "    opt='sgd',\n",
    "    no_epochs=no_epochs,\n",
    "    batch_size=64,\n",
    "    total_batch_size=64,\n",
    "    learning_rate=0.01,\n",
    "    weight='class_balanced',\n",
    "    dataset='mini',\n",
    "    scheduler='reduce_lr_on_plateau',\n",
    "    **loss_kwg,\n",
    "    **weight_kwg,\n",
    "    note=f'trained on {OPTION}',\n",
    "    **mixup_kwg\n",
    ")\n",
    "\n",
    "# include configuration from model\n",
    "_model_config = models.get_model(config.model, pretrained=False).pretrained_config\n",
    "config.update(_model_config)\n",
    "\n",
    "# save config file\n",
    "config.save(DATA_DIR + config.specs_name)\n",
    "\n",
    "# create loss, optimizer and scheduler functions\n",
    "loss_fn = loss_functions.LOSSES[config.loss]\n",
    "weight_fn = loss_functions.WEIGHTING[config.weight]\n",
    "opt_fn = training.OPTIMIZERS[config.opt]\n",
    "sched_fn = training.SCHEDULERS[config.scheduler]\n",
    "\n",
    "DATASETS = {\n",
    "    'full': ('DF20-train_metadata_PROD.csv', 'DF20-public_test_metadata_PROD.csv'),\n",
    "    'mini': ('DF20M-train_metadata_PROD.csv', 'DF20M-public_test_metadata_PROD.csv')\n",
    "}\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a211a-8ae4-45bb-81ba-acec8b917ace",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dde4f7-f937-4e60-b786-d32206f2845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load metadata\n",
    "train_df = pd.read_csv(DATA_DIR + DATASETS[config.dataset][0])\n",
    "valid_df = pd.read_csv(DATA_DIR + DATASETS[config.dataset][1])\n",
    "\n",
    "classes = np.unique(train_df['scientificName'])\n",
    "no_classes = len(classes)\n",
    "assert no_classes == len(np.unique(valid_df['scientificName']))\n",
    "print(f'No classes: {no_classes}')\n",
    "print(f'Train set length: {len(train_df):,d}')\n",
    "print(f'Validation set length: {len(valid_df):,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c515df5-d5f0-416e-a7a0-d16eae67a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# include bbox annotations to the training set\n",
    "bbox_df = pd.read_csv(BBOX_DATA_DIR + 'DFM_bbox_annotations.csv')\n",
    "bbox_df = bbox_df.drop_duplicates('ImageUniqueID')\n",
    "# assert len(bbox_df) >= len(train_df) + len(valid_df)\n",
    "train_df = train_df.merge(bbox_df, 'left', on='ImageUniqueID', validate='one_to_one')\n",
    "valid_df = valid_df.merge(bbox_df, 'left', on='ImageUniqueID', validate='one_to_one')\n",
    "\n",
    "# create bbox column\n",
    "for _df in [train_df, valid_df]:\n",
    "    _df['bbox'] = _df[['xmin', 'ymin', 'xmax', 'ymax']].apply(lambda r: data.BBox(*r.values), axis=1)\n",
    "    _df['bbox_area'] = _df['bbox'].apply(lambda bbox: bbox.area)\n",
    "\n",
    "# get stats about bounding boxes\n",
    "print('Mean bounding box area: {:.5f}'.format(train_df['bbox_area'].mean()))\n",
    "print('Median bounding box area: {:.5f}'.format(train_df['bbox_area'].median()))\n",
    "train_df['bbox_area'].plot(kind='hist', bins=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0a9ac-4b07-4b30-95a4-d5576fc24ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cdac4eb-d0cc-4c43-8434-ff44cc9a471c",
   "metadata": {},
   "source": [
    "## Create Cropping Image DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64171740-d83a-4814-a08d-28813a33e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "import PIL\n",
    "\n",
    "\n",
    "class CombinedImageCroppingDataset(data.ImageCroppingDataset):\n",
    "    def __getitem__(self, idx):\n",
    "        img, label, bbox = self.get_item(idx, crop=False, return_bbox=True)\n",
    "        img_cropped = self.crop_tfm(img, bbox)\n",
    "\n",
    "        resize_tfm = T.RandomResizedCrop(self.crop_tfm.size, scale=(0.8, 1.0))\n",
    "        img = resize_tfm(img)\n",
    "        img = self.transform_image(img)\n",
    "        img_cropped = self.transform_image(img_cropped)\n",
    "        img = torch.cat([img, img_cropped], dim=0)\n",
    "\n",
    "        if self._label2id:\n",
    "            label = self._label2id[label]\n",
    "        return img, label\n",
    "\n",
    "\n",
    "class MixupImageCroppingDataset(data.ImageCroppingDataset):\n",
    "    def __init__(self, df, img_path_col, label_col, bbox_col,\n",
    "                 transforms, path='.', labels=None, encode_labels=True,\n",
    "                 crop_p=1.0, alpha=0.5, mixup_p=0.25):\n",
    "        super().__init__(df, img_path_col, label_col, bbox_col, \n",
    "                         transforms, path, labels, encode_labels, crop_p)\n",
    "        self.alpha = alpha\n",
    "        self.mixup_p = mixup_p\n",
    "\n",
    "    def get_item(self, idx, *, crop=True, return_bbox=False):\n",
    "        img, label, bbox = super().get_item(idx, crop=False, return_bbox=True)\n",
    "        if crop:\n",
    "            if self.alpha == 0.0:\n",
    "                img = img.resize(self.size, resample=PIL.Image.BILINEAR)\n",
    "            elif self.alpha == 1.0:\n",
    "                img = self.crop_tfm(img, bbox)\n",
    "            else:\n",
    "                if self.mixup_p >= np.random.rand(1):\n",
    "                    img_crop = self.crop_tfm(img, bbox)\n",
    "                    img = img.resize(self.size, resample=PIL.Image.BILINEAR)\n",
    "                    # img = ((img * (1-self.alpha)) + (img_crop * self.alpha)).astype(int)\n",
    "                    img = PIL.Image.blend(img, img_crop, self.alpha)  # out = image1*(1.0 - alpha) + image2*alpha\n",
    "                else:\n",
    "                    img = self.crop_tfm(img, bbox)\n",
    "\n",
    "        return (img, label, bbox) if return_bbox else (img, label)\n",
    "\n",
    "\n",
    "def get_dataloader(dataset_cls, df, img_path_col, label_col, path='.', transforms=None,\n",
    "                   batch_size=32, shuffle=True, num_workers=4, sampler=None,\n",
    "                   bbox_col=None, labels=None, encode_labels=True, **kwargs):\n",
    "    dataset = dataset_cls(\n",
    "        df, img_path_col, label_col, path=path, bbox_col=bbox_col,\n",
    "        transforms=transforms, labels=labels, encode_labels=encode_labels, **kwargs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
    "                            num_workers=num_workers, sampler=sampler)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c869c1-f4ae-4fb2-b743-f71f0d4727ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust training data\n",
    "trainset_cls = data.ImageCroppingDataset\n",
    "validset_cls = data.ImageCroppingDataset\n",
    "train_crop_p = 1.0\n",
    "in_chans = 3\n",
    "if OPTION == 'crop':\n",
    "    pass\n",
    "elif OPTION == 'full and crop':\n",
    "    print(f'Original train df size: {len(train_df):,d}')\n",
    "    _train_df = train_df.copy()\n",
    "    _train_df['bbox'] = _train_df['bbox'].apply(lambda _: data.BBox(0, 0, 1, 1))\n",
    "    train_df = pd.concat([train_df, _train_df], axis=0)\n",
    "    print(f'Full and cropped train df size: {len(train_df):,d}')\n",
    "elif OPTION == 'full and crop (random)':\n",
    "    train_crop_p = 0.5\n",
    "elif OPTION == 'full and crop (6 channels)':\n",
    "    in_chans = 6\n",
    "    trainset_cls = CombinedImageCroppingDataset\n",
    "    validset_cls = CombinedImageCroppingDataset\n",
    "elif OPTION == 'full and crop (mixup)':\n",
    "    trainset_cls = MixupImageCroppingDataset\n",
    "\n",
    "print(f'Using \"{trainset_cls.__name__}\" trainng set class.')\n",
    "print(f'Using \"{validset_cls.__name__}\" validation set class.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e523c1-e158-4035-85a2-5bd1a065cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transforms\n",
    "train_tfms, valid_tfms = data.get_transforms(\n",
    "    size=config.input_size, mean=config.image_mean,\n",
    "    std=config.image_std)\n",
    "\n",
    "# create data loaders\n",
    "trainloader = get_dataloader(\n",
    "    trainset_cls, train_df, img_path_col='image_path', label_col='scientificName',\n",
    "    bbox_col='bbox', labels=classes,\n",
    "    path=DATA_DIR + TRAIN_SET_DIR, transforms=train_tfms,\n",
    "    batch_size=config.batch_size, shuffle=True, num_workers=4, **mixup_kwg)\n",
    "validloader = get_dataloader(\n",
    "    validset_cls, valid_df, img_path_col='image_path', label_col='scientificName',\n",
    "    bbox_col='bbox', labels=classes,\n",
    "    path=DATA_DIR + TRAIN_SET_DIR, transforms=valid_tfms,\n",
    "    batch_size=config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# set probability of cropping each image\n",
    "trainloader.dataset.crop_tfm.p = train_crop_p\n",
    "print(f'Cropping will be on random with probability {train_crop_p}')\n",
    "\n",
    "trainloader.dataset.show_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf70a4-1372-4197-b84c-8b51ff6b5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # validate functionality of the dataset\n",
    "# for img, label in validloader: break\n",
    "\n",
    "# print(img.shape)\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = viz.create_fig(ncols=3, colsize=3, rowsize=3)\n",
    "# validloader.dataset.show_item(1, ax=ax1)\n",
    "# viz.imshow(img[1, :3], ax=ax2)\n",
    "# viz.imshow(img[1, 3:], ax=ax3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a953ce-2923-49da-82c9-e85786b90fd9",
   "metadata": {},
   "source": [
    "### Review Cropping Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778212cf-2f75-4054-8216-458f1d441446",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader.dataset.show_items(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3511ce4-ab24-4003-a8d7-7513a10d2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader.dataset.show_items(50, apply_transforms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99864993-18e6-46d0-b909-5adb02ae0a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a4a0565-e092-43c4-b840-90875e747cb1",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9c1a9-b9fc-4adf-b59c-51263cae15dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "from src.utils.progress_log import ProgressBar\n",
    "\n",
    "\n",
    "class Trainer2(training.Trainer):\n",
    "    def train_epoch(self, optimizer, scheduler=None, pbar=None, return_preds=False):\n",
    "        model = self.model\n",
    "        dataloader = self.trainloader\n",
    "        validloader = self.validloader\n",
    "        criterion = self.criterion\n",
    "        accumulation_steps = self.accumulation_steps\n",
    "        device = self.device\n",
    "        if pbar is None:\n",
    "            pbar = ProgressBar().progress_bar\n",
    "\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        avg_loss = 0.\n",
    "        preds_all, targs_all = [], []\n",
    "        for i, (imgs, targs) in pbar(enumerate(dataloader), total=len(dataloader)):\n",
    "            imgs = imgs.to(device)\n",
    "            targs = targs.to(device)\n",
    "\n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, targs)\n",
    "            avg_loss += loss.item() / len(dataloader)\n",
    "\n",
    "            # scale the loss to the mean of the accumulated batch size\n",
    "            loss = loss / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # make optimizer step\n",
    "            if (i - 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                if scheduler is not None and not isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n",
    "                    scheduler.step()\n",
    "\n",
    "            if i == (len(trainloader) // 2):\n",
    "                preds, targs, valid_loss = self.predict(\n",
    "                        self.validloader, pbar=pbar)\n",
    "                # update ReduceLROnPlateau scheduler (if available)\n",
    "                if (valid_loss is not None and scheduler is not None\n",
    "                    and isinstance(scheduler, lr_scheduler.ReduceLROnPlateau)):\n",
    "                    scheduler.step(valid_loss)  # scores[self.primary_metric]\n",
    "\n",
    "            if return_preds:\n",
    "                preds_all.append(preds.detach().cpu().numpy())\n",
    "                targs_all.append(targs.detach().cpu().numpy())\n",
    "\n",
    "        if return_preds:\n",
    "            preds_all = np.concatenate(preds_all, axis=0)\n",
    "            targs_all = np.concatenate(targs_all, axis=0)\n",
    "        else:\n",
    "            preds_all, targs_all = None, None\n",
    "        return preds_all, targs_all, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89bf47a-987c-4573-b71c-9ccff71b9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = models.get_model(config.model, no_classes, pretrained=True, in_chans=in_chans)\n",
    "assert np.all([param.requires_grad for param in model.parameters()])\n",
    "\n",
    "# create loss\n",
    "freq = train_df['scientificName'].value_counts()[trainloader.dataset.labels].values\n",
    "weights = weight_fn(freq, **weight_kwg)\n",
    "criterion = loss_fn(weight=torch.Tensor(weights).to(device) if weights is not None else None,\n",
    "                    **loss_kwg)\n",
    "\n",
    "# create trainer\n",
    "trainer_cls = training.Trainer\n",
    "if OPTION == 'full and crop':\n",
    "    trainer_cls = Trainer2\n",
    "trainer = trainer_cls(\n",
    "    model,\n",
    "    trainloader,\n",
    "    criterion,\n",
    "    opt_fn,\n",
    "    sched_fn,\n",
    "    validloader=validloader,\n",
    "    accumulation_steps=config.total_batch_size // config.batch_size,\n",
    "    path=DATA_DIR,\n",
    "    # model_filename=config.model_name,\n",
    "    # history_filename=config.history_file,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742b11c-430f-48e3-a93d-ef906b73ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer.train(no_epochs=config.no_epochs, lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5c0f1-0560-4164-bc24-aaa08df58f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5dc1ba-3550-42ff-b234-04c5a7ed0323",
   "metadata": {},
   "source": [
    "## Create Predictions on Full Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8354ed6-e184-4751-a4fb-780a01da08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validloader with full images\n",
    "validloader_full = data.get_dataloader(\n",
    "    valid_df, img_path_col='image_path', label_col='scientificName',\n",
    "    path=DATA_DIR + TRAIN_SET_DIR, transforms=valid_tfms, labels=classes,\n",
    "    batch_size=config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# load fine-tuned checkpoint\n",
    "training.load_model(model, config.model_name, path=DATA_DIR + 'models')\n",
    "\n",
    "# create predictions\n",
    "pred, targ, _ = training.predict(model, validloader_full)\n",
    "\n",
    "# compute scores\n",
    "scores = training.classification_scores(pred, targ)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36050f0d-b2c4-4315-9a62-330a283b4074",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.progress_log import CSVProgress\n",
    "\n",
    "# save scores\n",
    "csv_progress = CSVProgress(f'{config.model_name}_full.csv', DATA_DIR)\n",
    "_scores = {'epoch': None, 'train_loss': None, 'valid_loss': None,\n",
    "           **scores, 'time': None}\n",
    "csv_progress.log_epoch_scores(_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07213b20-1339-4235-97f0-5c0ab82ea308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
